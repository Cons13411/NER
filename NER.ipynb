{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import io\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import itertools\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(vocab_path, tags_path):\n",
    "    vocab = {}\n",
    "    with open(vocab_path) as f:\n",
    "        for i, l in enumerate(f.read().splitlines()):\n",
    "            vocab[l] = i  # to avoid the 0\n",
    "        # loading tags (we require this to map tags to their indices)\n",
    "    vocab['<PAD>'] = len(vocab) # 35180\n",
    "    tag_map = {}\n",
    "    with open(tags_path) as f:\n",
    "        for i, t in enumerate(f.read().splitlines()):\n",
    "            tag_map[t] = i \n",
    "    \n",
    "    return vocab, tag_map\n",
    "\n",
    "def get_params(vocab, tag_map, sentences_file, labels_file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(sentences_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each token by its index if it is in vocab\n",
    "            # else use index of UNK_WORD\n",
    "            s = [vocab[token] if token in vocab \n",
    "                 else vocab['UNK']\n",
    "                 for token in sentence.split(' ')]\n",
    "            sentences.append(s)\n",
    "\n",
    "    with open(labels_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each label by its index\n",
    "            l = [tag_map[label] for label in sentence.split(' ')] # I added plus 1 here\n",
    "            labels.append(l) \n",
    "    return sentences, labels, len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = get_vocab('./data/utf-8\\'\\'words.txt', './data/utf-8\\'\\'tags.txt')\n",
    "tr_sentence, tr_labels, len_train = get_params(words, tags, './data/train/utf-8\\'\\'sentences.txt', './data/train/utf-8\\'\\'labels.txt')\n",
    "e_sentence, e_labels, len_eval = get_params(words, tags, './data/eval/utf-8\\'\\'sentences.txt', './data/eval/utf-8\\'\\'labels.txt')\n",
    "te_sentence, te_labels, len_test = get_params(words, tags, './data/test/utf-8\\'\\'sentences.txt', './data/test/utf-8\\'\\'labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print word['the']: 9\n",
      "print word['<PAD>']: 35180\n"
     ]
    }
   ],
   "source": [
    "print(f\"print word['the']: {words['the']}\")\n",
    "print(f\"print word['<PAD>']: {words['<PAD>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n"
     ]
    }
   ],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of outputs is tag_map 17\n",
      "Num of vocabulary words: 35181\n",
      "The vocab size is 35181\n",
      "The training size is 33570\n",
      "The validation size is 7194\n",
      "An example of the first sentence is [22, 1, 23, 24, 11, 9, 25, 26, 9, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 35, 13, 35, 40, 9, 41, 21, 35]\n",
      "An example of its corresponding label is [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print('The number of outputs is tag_map', len(tags))\n",
    "\n",
    "g_vocab_size = len(words)\n",
    "print(f\"Num of vocabulary words: {g_vocab_size}\")\n",
    "print('The vocab size is', len(words))\n",
    "print('The training size is', len_train)\n",
    "print('The validation size is', len_eval)\n",
    "print('An example of the first sentence is', tr_sentence[1])\n",
    "print('An example of its corresponding label is', tr_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, tags, max_length, pading):\n",
    "        \n",
    "        self.instances = []\n",
    "        for sentence, tag in zip(sentences,tags):\n",
    "            padded_sentece = np.full((1,max_length), pading )\n",
    "            padded_sentece[:,:len(sentence)] = sentence\n",
    "            padded_tag = np.full((1,max_length), pading )\n",
    "            padded_tag[:,:len(sentence)] = tag\n",
    "            self.instances.append((padded_sentece,padded_tag))\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sentence, tag = self.instances[index]\n",
    "        sentence = torch.tensor(sentence).view(1,-1)\n",
    "        tag = torch.tensor(tag).view(1,-1)\n",
    "        return sentence, tag\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.instances)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "max_length = 0 \n",
    "for sentence1, sentence2, sentence3 in itertools.zip_longest(tr_sentence,e_sentence,te_sentence, fillvalue = [] ):\n",
    "    length = np.max((len(sentence1),len(sentence2), len(sentence3)))\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[   49,   126,     9,   533,   686,  2892,  1002,    34, 12794,    29,\n",
      "            243,  6852, 12568,    63,  6836,  4289,    11,     9,   309,    21,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180]],\n",
      "\n",
      "        [[  272,    78,  4076, 12236,  3534,  4796,     1, 12237,    93,  1346,\n",
      "             21, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180]]])\n",
      "tensor([[[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180]],\n",
      "\n",
      "        [[    0,     0,     0,     0,     0,     0,     0,     1,     0,     1,\n",
      "              0, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
      "          35180, 35180, 35180, 35180]]])\n"
     ]
    }
   ],
   "source": [
    "data = data_generator(tr_sentence, tr_labels, max_length, words['<PAD>'])\n",
    "train_loader = DataLoader(data, batch_size=2, shuffle=True)\n",
    "x,y = next(iter(train_loader))\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNER(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, vocab_size, num_tags,  hidden, num_layer, embedding):\n",
    "        super(LSTMNER, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_tags = num_tags\n",
    "        self.embedding = embedding\n",
    "        self.num_hidden = hidden\n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.embed_layer = nn.Embedding(self.vocab_size, self.embedding)\n",
    "        self.LSTM = nn.LSTM(self.embedding, self.num_hidden, self.num_layer, batch_first = True)\n",
    "        self.decoder = nn.Linear(self.num_hidden, self.num_tags)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        embed = self.embed_layer(x)\n",
    "\n",
    "        output, hidden = self.LSTM(embed.unsqueeze(1), hidden)\n",
    "        preds = self.decoder(output)\n",
    "        return preds, hidden \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_state = torch.randn(self.num_layer,batch_size,self.num_hidden)\n",
    "        cell_state = torch.randn(self.num_layer,batch_size,self.num_hidden)\n",
    "        hidden = (hidden_state,cell_state)\n",
    "        return hidden\n",
    "    \n",
    "    def evaluate(self, eval_loader, criterion):\n",
    "        \n",
    "        model.train(mode = False)\n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            total = 0\n",
    "            total_correct = 0\n",
    "            total_words = 0\n",
    "            \n",
    "\n",
    "            for sentence, label in eval_loader:\n",
    "                model.zero_grad()\n",
    "                sentence = torch.squeeze(sentence,dim=1)\n",
    "                label = torch.squeeze(label, dim=1)\n",
    "\n",
    "                \n",
    "                batch_loss = 0\n",
    "                if len(sentence.size()) == 1:\n",
    "                    continue\n",
    "\n",
    "                batch_size = sentence.size(0)\n",
    "                hidden = model.init_hidden(batch_size)\n",
    "                \n",
    "                for word_id in range(sentence.size(1)):\n",
    "                    \n",
    "                    output, hidden = model(sentence[:,word_id],hidden)\n",
    "                    tg_label = label[:,word_id]\n",
    "                    batch_loss += criterion(torch.squeeze(output,dim=1), tg_label)\n",
    "                    \n",
    "                    total_words += np.sum(np.array([1 if tg_label[i] != words['<PAD>'] else 0 for i in range(batch_size)]))\n",
    "                    total_correct += torch.sum(np.argmax(output.squeeze(),axis=-1) == tg_label)\n",
    "                    \n",
    "                avg_loss = batch_loss.item()/sentence.size(1)\n",
    "                losses.append(avg_loss)\n",
    "                total += 1\n",
    "                \n",
    "            accuracy = float(total_correct)/total_words\n",
    "            epoch_loss = sum(losses) / total\n",
    "            \n",
    "            return epoch_loss, accuracy\n",
    "        \n",
    "    def test_ner(self, test_loader):\n",
    "            \n",
    "            model.train(mode = False)\n",
    "            with torch.no_grad():\n",
    "\n",
    "                total_correct = 0\n",
    "                total_words = 0\n",
    "\n",
    "\n",
    "                for sentence, label in test_loader:\n",
    "                    model.zero_grad()\n",
    "                    sentence = torch.squeeze(sentence,dim=1)\n",
    "                    label = torch.squeeze(label, dim=1)\n",
    "\n",
    "                    if len(sentence.size()) == 1:\n",
    "                        continue\n",
    "\n",
    "                    batch_size = sentence.size(0)\n",
    "                    hidden = model.init_hidden(batch_size)\n",
    "\n",
    "                    for word_id in range(sentence.size(1)):\n",
    "\n",
    "                        output, hidden = model(sentence[:,word_id],hidden)\n",
    "                        tg_label = label[:,word_id]\n",
    "\n",
    "                        total_words += np.sum(np.array([1 if tg_label[i] != words['<PAD>'] else 0 for i in range(batch_size)]))\n",
    "                        total_correct += torch.sum(np.argmax(output.squeeze(),axis=-1) == tg_label)\n",
    "\n",
    "\n",
    "                accuracy = float(total_correct)/total_words\n",
    "\n",
    "                return accuracy\n",
    "            \n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        \n",
    "        return True\n",
    "        \n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples from the dataset: 33570\n",
      "Batch size (a power of 2): 64\n",
      "Number of steps to cover one epoch: 524\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(tr_sentence)\n",
    "batch_size = 64\n",
    "print('Number of samples from the dataset:', num_samples)\n",
    "print('Batch size (a power of 2):', int(batch_size))\n",
    "steps_per_epoch = int(num_samples/batch_size)\n",
    "print('Number of steps to cover one epoch:', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_generator(tr_sentence, tr_labels, max_length, words['<PAD>'])\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "eval_data = data_generator(e_sentence, e_labels, max_length, words['<PAD>'])\n",
    "eval_loader = DataLoader(eval_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, train_loader, optim):\n",
    "    \n",
    "    losses = []\n",
    "    total = 0\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for sentence, label in train_loader:\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.train(mode = True)\n",
    "        sentence = sentence.squeeze()\n",
    "        label = label.squeeze()\n",
    "        \n",
    "        batch_loss = 0\n",
    "        if len(sentence.size()) == 1:\n",
    "            continue\n",
    "                \n",
    "        batch_size = sentence.size(0)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        for word_id in range(sentence.size(1)):\n",
    "            \n",
    "            output, hidden = model(sentence[:,word_id],hidden)\n",
    "            tg_label = label[:,word_id]\n",
    "            batch_loss += criterion(output.squeeze(), tg_label)\n",
    "            converted_output = output.detach()\n",
    "            \n",
    "            total_words += np.sum(np.array([1 if tg_label[i] != words['<PAD>'] else 0 for i in range(batch_size)]))\n",
    "            total_correct += torch.sum(np.argmax(converted_output.squeeze(),axis=-1) == tg_label)\n",
    "            \n",
    "        batch_loss.backward()\n",
    "        optim.step()\n",
    "        avg_loss = batch_loss.item()/sentence.size(1)\n",
    "        losses.append(avg_loss)\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = float(total_correct)/total_words\n",
    "    \n",
    "    return losses, total, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep    0 |Train loss 0.328526 |Train Acc 0.813266 |Evaluation loss 0.199674 |Evaluation Acc 0.859466|Saved\n",
      "\n",
      "Ep    1 |Train loss 0.170322 |Train Acc 0.884685 |Evaluation loss 0.140305 |Evaluation Acc 0.904519|Saved\n",
      "\n",
      "Ep    2 |Train loss 0.124468 |Train Acc 0.915822 |Evaluation loss 0.109306 |Evaluation Acc 0.922637|Saved\n",
      "\n",
      "Ep    3 |Train loss 0.100078 |Train Acc 0.931342 |Evaluation loss 0.093730 |Evaluation Acc 0.933803|Saved\n",
      "\n",
      "Ep    4 |Train loss 0.083213 |Train Acc 0.941031 |Evaluation loss 0.082697 |Evaluation Acc 0.939545|Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "model = LSTMNER(batch_size, len(words), len(tags), 50, 1, 50)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=words['<PAD>'])\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0, betas=(0.9, 0.999),\n",
    "                         eps=1e-8, amsgrad=False)\n",
    "\n",
    "epoch = 5\n",
    "least_cost = np.inf\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    print('Ep {:4d}'.format(i), end='')\n",
    "    \n",
    "    losses, total, tr_accuracy = train_network(model, train_loader, optim)\n",
    "    \n",
    "    epoch_loss = np.sum(losses)/total\n",
    "    train_loss.append(epoch_loss)\n",
    "    \n",
    "    print(' |Train loss {:4f}'.format(epoch_loss), end='')\n",
    "    print(' |Train Acc {:4f}'.format(tr_accuracy), end='')\n",
    "    evaluate_loss, accuracy = model.evaluate(eval_loader, criterion)\n",
    "    eval_loss.append(evaluate_loss)\n",
    "    print(' |Evaluation loss {:4f}'.format(evaluate_loss), end='')\n",
    "    print(' |Evaluation Acc {:4f}'.format(accuracy), end='')\n",
    "    \n",
    "    if least_cost > evaluate_loss :\n",
    "        least_loss = evaluate_loss\n",
    "        torch.save(model.state_dict(), './NER.pth')\n",
    "        best_model = copy.deepcopy(model)\n",
    "        print('|Saved\\n')\n",
    "    else:\n",
    "        print('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 0.938944147880743\n"
     ]
    }
   ],
   "source": [
    "test_data = data_generator(te_sentence, te_labels, max_length, words['<PAD>'])\n",
    "test_loader = DataLoader(test_data, batch_size=len(te_sentence), shuffle=True)\n",
    "accuracy = best_model.test_ner(test_loader)\n",
    "print(f\"Accuracy of the model on test data: {accuracy}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model,sentence):\n",
    "    hidden = model.init_hidden(1)\n",
    "    vec = [words[token] if token in words else words['UNK'] for token in sentence.split(' ')]\n",
    "    batch_data = np.zeros((1,len(vec)))\n",
    "    batch_data[0][:] = vec\n",
    "    sentence_vec = torch.tensor(batch_data, dtype= int)\n",
    "    labels = list(tags.keys())\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for id_word in range(len(vec)):\n",
    "            output, hidden = model(sentence_vec[:,id_word], hidden)\n",
    "            pred.append(labels[int(np.argmax(output, axis= -1))])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Ali is going on vacation in December to New York'\n",
    "pred = prediction(best_model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali\tB-per\n",
      "is\tO\n",
      "going\tO\n",
      "on\tO\n",
      "vacation\tO\n",
      "in\tO\n",
      "December\tB-tim\n",
      "to\tO\n",
      "New\tB-geo\n",
      "York\tI-geo\n"
     ]
    }
   ],
   "source": [
    "for word, label in zip(sentence.split(' '), pred):\n",
    "    print(f\"{word}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
